{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce4528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1349bf",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300f982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "–ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LIGHTGBM\n",
      "============================================================\n",
      "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\n",
      "3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤...\n",
      "   –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ 702 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
      "   –ü—Ä–∏–º–µ—Ä: 'SK_ID_CURR' -> 'SK_ID_CURR'\n",
      "4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...\n",
      "   –ë–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 115 -> int8\n",
      "   –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 587 -> float32\n",
      "5. –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π...\n",
      "   –£–¥–∞–ª–µ–Ω–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π\n",
      "6. –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...\n",
      "\n",
      "============================================================\n",
      "–ò–¢–û–ì–û–í–´–ï –†–ê–ó–ú–ï–†–´ –î–ê–ù–ù–´–•\n",
      "============================================================\n",
      "X_train: (184506, 697) (–ø–∞–º—è—Ç—å: 432.5 MB)\n",
      "X_val:   (61502, 697) (–ø–∞–º—è—Ç—å: 144.2 MB)\n",
      "y_train: (184506,) (—Ç–∏–ø: int8)\n",
      "y_val:   (61502,) (—Ç–∏–ø: int8)\n",
      "\n",
      "–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ X_train:\n",
      "  float32: 587 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
      "  int8: 110 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
      "\n",
      "–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:\n",
      "  X_train: 0\n",
      "  X_val:   0\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä—ã –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
      "  ['SK_ID_CURR', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL']\n",
      "\n",
      "============================================================\n",
      "–î–ê–ù–ù–´–ï –ì–û–¢–û–í–´ –î–õ–Ø LIGHTGBM!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"–ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø LIGHTGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "train_df = pd.read_csv('./data/train_processed.csv')\n",
    "val_df = pd.read_csv('./data/val_processed.csv')\n",
    "\n",
    "# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ç–∞—Ä–≥–µ—Ç\n",
    "print(\"2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "X_train = train_df.drop(columns=['TARGET'])\n",
    "y_train = train_df['TARGET']\n",
    "X_val = val_df.drop(columns=['TARGET'])\n",
    "y_val = val_df['TARGET']\n",
    "\n",
    "# 3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è LightGBM\n",
    "print(\"3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤...\")\n",
    "\n",
    "def fix_column_names(df):\n",
    "    \"\"\"–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å LightGBM\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    new_columns = {}\n",
    "    \n",
    "    for col in df_fixed.columns:\n",
    "        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ\n",
    "        new_name = str(col)\n",
    "        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã\n",
    "        for char in [' ', '(', ')', '[', ']', '{', '}', '/', '\\\\', ':', ',', '.', '-', '+', '*', '&', '^', '%', '$', '#', '@', '!', '?']:\n",
    "            new_name = new_name.replace(char, '_')\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è\n",
    "        while '__' in new_name:\n",
    "            new_name = new_name.replace('__', '_')\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ\n",
    "        new_name = new_name.strip('_')\n",
    "        \n",
    "        # –ï—Å–ª–∏ –∏–º—è –ø—É—Å—Ç–æ–µ –∏–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ü–∏—Ñ—Ä—ã\n",
    "        if not new_name:\n",
    "            new_name = f'feature_{col}'\n",
    "        elif new_name[0].isdigit():\n",
    "            new_name = f'f_{new_name}'\n",
    "        \n",
    "        new_columns[col] = new_name\n",
    "    \n",
    "    return df_fixed.rename(columns=new_columns)\n",
    "\n",
    "X_train = fix_column_names(X_train)\n",
    "X_val = fix_column_names(X_val)\n",
    "\n",
    "print(f\"   –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ {X_train.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
    "print(f\"   –ü—Ä–∏–º–µ—Ä: '{list(train_df.columns)[0]}' -> '{X_train.columns[0]}'\")\n",
    "\n",
    "# 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "# –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è -> int8\n",
    "y_train = y_train.astype(np.int8)\n",
    "y_val = y_val.astype(np.int8)\n",
    "\n",
    "# –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (0/1) -> int8\n",
    "binary_mask = (X_train.nunique() <= 2) & (X_train.isin([0, 1]).all())\n",
    "binary_cols = X_train.columns[binary_mask].tolist()\n",
    "\n",
    "if binary_cols:\n",
    "    X_train[binary_cols] = X_train[binary_cols].astype(np.int8)\n",
    "    X_val[binary_cols] = X_val[binary_cols].astype(np.int8)\n",
    "    print(f\"   –ë–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(binary_cols)} -> int8\")\n",
    "\n",
    "# –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ -> float32\n",
    "other_cols = [col for col in X_train.columns if col not in binary_cols]\n",
    "if other_cols:\n",
    "    X_train[other_cols] = X_train[other_cols].astype(np.float32)\n",
    "    X_val[other_cols] = X_val[other_cols].astype(np.float32)\n",
    "    print(f\"   –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(other_cols)} -> float32\")\n",
    "\n",
    "# 5. –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π\n",
    "print(\"5. –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π...\")\n",
    "zero_var_cols = X_train.columns[X_train.std() == 0]\n",
    "if len(zero_var_cols) > 0:\n",
    "    X_train = X_train.drop(columns=zero_var_cols)\n",
    "    X_val = X_val.drop(columns=zero_var_cols)\n",
    "    print(f\"   –£–¥–∞–ª–µ–Ω–æ {len(zero_var_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π\")\n",
    "else:\n",
    "    print(\"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\")\n",
    "\n",
    "# 6. –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏\n",
    "print(\"6. –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...\")\n",
    "del train_df, val_df\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 7. –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–ò–¢–û–ì–û–í–´–ï –†–ê–ó–ú–ï–†–´ –î–ê–ù–ù–´–•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"X_train: {X_train.shape} (–ø–∞–º—è—Ç—å: {X_train.memory_usage().sum()/1024**2:.1f} MB)\")\n",
    "print(f\"X_val:   {X_val.shape} (–ø–∞–º—è—Ç—å: {X_val.memory_usage().sum()/1024**2:.1f} MB)\")\n",
    "print(f\"y_train: {y_train.shape} (—Ç–∏–ø: {y_train.dtype})\")\n",
    "print(f\"y_val:   {y_val.shape} (—Ç–∏–ø: {y_val.dtype})\")\n",
    "\n",
    "print(f\"\\n–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ X_train:\")\n",
    "for dtype in X_train.dtypes.unique():\n",
    "    count = (X_train.dtypes == dtype).sum()\n",
    "    print(f\"  {dtype}: {count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "\n",
    "print(f\"\\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:\")\n",
    "print(f\"  X_train: {X_train.isna().sum().sum()}\")\n",
    "print(f\"  X_val:   {X_val.isna().sum().sum()}\")\n",
    "\n",
    "print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "print(f\"  {list(X_train.columns[:3])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–î–ê–ù–ù–´–ï –ì–û–¢–û–í–´ –î–õ–Ø LIGHTGBM!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffc6ae",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16b670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "–°–¢–ï–ö–ò–ù–ì (STACKING) –ù–ê –û–°–ù–û–í–ï –õ–£–ß–®–ò–• –ú–û–î–ï–õ–ï–ô\n",
      "================================================================================\n",
      "\n",
      "1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...\n",
      "‚úÖ Random Forest –∑–∞–≥—Ä—É–∂–µ–Ω (–∏–∑ dict)\n",
      "‚úÖ LightGBM –∑–∞–≥—Ä—É–∂–µ–Ω\n",
      "‚úÖ XGBoost –∑–∞–≥—Ä—É–∂–µ–Ω\n",
      "‚úÖ CatBoost –∑–∞–≥—Ä—É–∂–µ–Ω\n",
      "\n",
      "2. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –∞–Ω—Å–∞–º–±–ª—è...\n",
      "   –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: ['random_forest', 'lightgbm', 'xgboost', 'catboost']\n",
      "   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: LogisticRegression\n",
      "   –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: 5 —Ñ–æ–ª–¥–æ–≤\n",
      "\n",
      "3. –û–±—É—á–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª–∏...\n",
      "   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: (184506, 697)\n",
      "   –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...\n",
      "\n",
      "‚úÖ –°—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∑–∞ 57.6 –º–∏–Ω—É—Ç\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"–°–¢–ï–ö–ò–ù–ì (STACKING) –ù–ê –û–°–ù–û–í–ï –õ–£–ß–®–ò–• –ú–û–î–ï–õ–ï–ô\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ —Ñ–∞–π–ª–æ–≤\n",
    "print(\"\\n1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...\")\n",
    "\n",
    "# Random Forest\n",
    "rf_artifacts = joblib.load('./models/random_forest_best_model.pkl')\n",
    "rf_model = rf_artifacts['model']\n",
    "print(f\"‚úÖ Random Forest –∑–∞–≥—Ä—É–∂–µ–Ω (–∏–∑ {type(rf_artifacts).__name__})\")\n",
    "\n",
    "# LightGBM\n",
    "lgb_artifacts = joblib.load('./models/lgb_final_model.pkl')\n",
    "lgb_model = lgb_artifacts['model']\n",
    "print(f\"‚úÖ LightGBM –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_artifacts = joblib.load('./models/xgb_final_model.pkl')\n",
    "xgb_model = xgb_artifacts['model']\n",
    "print(f\"‚úÖ XGBoost –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "# CatBoost\n",
    "cat_artifacts = joblib.load('./models/catboost_final_model_with_metrics.pkl')\n",
    "cat_model = cat_artifacts['model']\n",
    "print(f\"‚úÖ CatBoost –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞\n",
    "print(\"\\n2. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –∞–Ω—Å–∞–º–±–ª—è...\")\n",
    "\n",
    "base_estimators = [\n",
    "    ('random_forest', rf_model),\n",
    "    ('lightgbm', lgb_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('catboost', cat_model)\n",
    "]\n",
    "\n",
    "# –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å (–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "meta_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª–∏\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,                # 5-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏\n",
    "    stack_method='predict_proba',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "    n_jobs=-1,            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è\n",
    "    passthrough=False     # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    ")\n",
    "\n",
    "print(f\"   –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: {[name for name, _ in base_estimators]}\")\n",
    "print(f\"   –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: LogisticRegression\")\n",
    "print(f\"   –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: 5 —Ñ–æ–ª–¥–æ–≤\")\n",
    "\n",
    "# 3. –û–±—É—á–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª–∏\n",
    "print(\"\\n3. –û–±—É—á–µ–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª–∏...\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X_train.shape}\")\n",
    "print(\"   –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...\")\n",
    "\n",
    "train_start = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "print(f\"\\n‚úÖ –°—Ç–µ–∫–∏–Ω–≥ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∑–∞ {train_time/60:.1f} –º–∏–Ω—É—Ç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bfd52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "–ë–õ–û–ö 2: –û–¶–ï–ù–ö–ê –ú–ï–¢–†–ò–ö –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò\n",
      "================================================================================\n",
      "\n",
      "1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 250 out of 250 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 250 out of 250 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫...\n",
      "\n",
      "============================================================\n",
      "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò\n",
      "============================================================\n",
      "\n",
      "–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n",
      "  Accuracy:   0.7081\n",
      "  F1 Macro:   0.5478\n",
      "  ROC-AUC:    0.7754\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82     56537\n",
      "           1       0.17      0.70      0.28      4965\n",
      "\n",
      "    accuracy                           0.71     61502\n",
      "   macro avg       0.57      0.70      0.55     61502\n",
      "weighted avg       0.90      0.71      0.77     61502\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40079 16458]\n",
      " [ 1497  3468]]\n",
      "\n",
      "================================================================================\n",
      "–ü–ï–†–ï–•–û–î –ö –ë–õ–û–ö–£ 3: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"–ë–õ–û–ö 2: –û–¶–ï–ù–ö–ê –ú–ï–¢–†–ò–ö –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "print(\"\\n1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...\")\n",
    "y_pred = stacking_model.predict(X_val)\n",
    "y_pred_proba = stacking_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 2. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
    "print(\"\\n2. –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫...\")\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "# 3. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "print(f\"  Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"  F1 Macro:   {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:    {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–ü–ï–†–ï–•–û–î –ö –ë–õ–û–ö–£ 3: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44441a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "–ë–õ–û–ö 3: –°–û–•–†–ê–ù–ï–ù–ò–ï –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò –° –ú–ï–¢–†–ò–ö–ê–ú–ò\n",
      "================================================================================\n",
      "\n",
      "1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è...\n",
      "   –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:\n",
      "   - –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: –¥–∞\n",
      "   - –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 5 —à—Ç\n",
      "   - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö: 5 —à—Ç\n",
      "\n",
      "2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª...\n",
      "   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: ./models/stacking_final_model.pkl\n",
      "   üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 355.0 MB\n",
      "\n",
      "================================================================================\n",
      "–ò–¢–û–ì–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò\n",
      "================================================================================\n",
      "\n",
      "–ú–æ–¥–µ–ª—å: Stacking Classifier\n",
      "–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: random_forest, lightgbm, xgboost, catboost\n",
      "–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: LogisticRegression\n",
      "–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 57.6 –º–∏–Ω—É—Ç\n",
      "\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ:\n",
      "  Accuracy:   0.7081\n",
      "  F1 Macro:   0.5478\n",
      "  ROC-AUC:    0.7754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40079 16458]\n",
      " [ 1497  3468]]\n",
      "\n",
      "================================================================================\n",
      "–°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–¨ –£–°–ü–ï–®–ù–û –°–û–•–†–ê–ù–ï–ù–ê!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"–ë–õ–û–ö 3: –°–û–•–†–ê–ù–ï–ù–ò–ï –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò –° –ú–ï–¢–†–ò–ö–ê–ú–ò\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "print(\"\\n1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è...\")\n",
    "\n",
    "stacking_artifacts = {\n",
    "    # –°–∞–º–∞ –º–æ–¥–µ–ª—å\n",
    "    'model': stacking_model,\n",
    "    \n",
    "    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö\n",
    "    'base_models': [name for name, _ in base_estimators],\n",
    "    'meta_model': 'LogisticRegression',\n",
    "    \n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "    'training_time_minutes': train_time / 60 if 'train_time' in locals() else None,\n",
    "    \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    'val_metrics': {\n",
    "        'accuracy': round(accuracy, 4),\n",
    "        'f1_macro': round(f1, 4),\n",
    "        'roc_auc': round(roc_auc, 4),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'classification_report': classification_report(y_val, y_pred, zero_division=0, output_dict=True)\n",
    "    },\n",
    "    \n",
    "    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö\n",
    "    'data_info': {\n",
    "        'feature_count': X_train.shape[1],\n",
    "        'training_samples': len(X_train),\n",
    "        'validation_samples': len(X_val),\n",
    "        'class_distribution_train': y_train.value_counts().to_dict(),\n",
    "        'class_distribution_val': y_val.value_counts().to_dict()\n",
    "    },\n",
    "    \n",
    "    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞\n",
    "    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model_type': 'StackingClassifier'\n",
    "}\n",
    "\n",
    "print(\"   –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:\")\n",
    "print(f\"   - –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: –¥–∞\")\n",
    "print(f\"   - –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(stacking_artifacts['val_metrics'])} —à—Ç\")\n",
    "print(f\"   - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö: {len(stacking_artifacts['data_info'])} —à—Ç\")\n",
    "\n",
    "# 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª\n",
    "print(\"\\n2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª...\")\n",
    "\n",
    "output_path = './models/stacking_final_model.pkl'\n",
    "joblib.dump(stacking_artifacts, output_path)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞\n",
    "file_size = os.path.getsize(output_path) / 1024**2\n",
    "print(f\"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}\")\n",
    "print(f\"   üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.1f} MB\")\n",
    "\n",
    "# 3. –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–ò–¢–û–ì–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–ò\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n–ú–æ–¥–µ–ª—å: Stacking Classifier\")\n",
    "print(f\"–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏: {', '.join([name for name, _ in base_estimators])}\")\n",
    "print(f\"–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å: LogisticRegression\")\n",
    "print(f\"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_time/60:.1f} –º–∏–Ω—É—Ç\" if 'train_time' in locals() else \"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: N/A\")\n",
    "\n",
    "print(f\"\\n–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ:\")\n",
    "print(f\"  Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"  F1 Macro:   {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:    {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"–°–¢–ï–ö–ò–ù–ì –ú–û–î–ï–õ–¨ –£–°–ü–ï–®–ù–û –°–û–•–†–ê–ù–ï–ù–ê!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

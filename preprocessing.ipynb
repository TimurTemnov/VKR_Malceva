{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676dabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import gc\n",
    "import pickle\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da774f29",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f80999f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "Размеры датасетов:\n",
      "Train: (184506, 1337)\n",
      "Val: (61502, 1337)\n",
      "Test: (61503, 1337)\n",
      "\n",
      "Колонок: 1337\n",
      "ID колонка: True\n",
      "Target колонка: True\n",
      "\n",
      "Распределение TARGET в train:\n",
      "0: 91.93% (169611 записей)\n",
      "1: 8.07% (14895 записей)\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "print(\"Загрузка данных...\")\n",
    "train = pd.read_csv('./data/train_raw.csv')\n",
    "val = pd.read_csv('./data/val_raw.csv')\n",
    "test = pd.read_csv('./data/test_raw.csv')\n",
    "\n",
    "print(f\"Размеры датасетов:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Val: {val.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "print(f\"\\nКолонок: {train.shape[1]}\")\n",
    "print(f\"ID колонка: {'SK_ID_CURR' in train.columns}\")\n",
    "print(f\"Target колонка: {'TARGET' in train.columns}\")\n",
    "\n",
    "# Проверяем распределение таргета\n",
    "if 'TARGET' in train.columns:\n",
    "    target_dist = train['TARGET'].value_counts(normalize=True)\n",
    "    print(f\"\\nРаспределение TARGET в train:\")\n",
    "    print(f\"0: {target_dist[0]:.2%} ({train['TARGET'].value_counts()[0]} записей)\")\n",
    "    print(f\"1: {target_dist[1]:.2%} ({train['TARGET'].value_counts()[1]} записей)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d091bdd",
   "metadata": {},
   "source": [
    "### Отделяем таргет от признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f97a361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (184506, 1335)\n",
      "Train target shape: (184506,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраняем ID и target отдельно\n",
    "train_ids = train['SK_ID_CURR']\n",
    "val_ids = val['SK_ID_CURR']\n",
    "test_ids = test['SK_ID_CURR']\n",
    "\n",
    "train_target = train['TARGET']\n",
    "val_target = val['TARGET']\n",
    "test_target = test['TARGET']\n",
    "\n",
    "# Удаляем из features\n",
    "X_train = train.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "X_val = val.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "X_test = test.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "\n",
    "print(f\"Features shape: {X_train.shape}\")\n",
    "print(f\"Train target shape: {train_target.shape}\")\n",
    "\n",
    "del train, val, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0c7a0",
   "metadata": {},
   "source": [
    "## Удаление столбцов с большим числом пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a900ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Колонок с >50.0% пропусков в train: 483\n",
      "\n",
      "Топ-10 колонок с наибольшим процентом пропусков:\n",
      "  client_credit_AMT_PAYMENT_CURRENT_min_min: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_min_max: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_min_mean: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_mean_mean: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_mean_min: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_mean_max: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_max_mean: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_max_min: 80.1%\n",
      "  client_credit_AMT_PAYMENT_CURRENT_max_max: 80.1%\n",
      "  client_credit_CNT_DRAWINGS_POS_CURRENT_max_mean: 80.1%\n",
      "\n",
      "Размеры после удаления:\n",
      "Train: (184506, 852)\n",
      "Val: (61502, 852)\n",
      "Test: (61503, 852)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_high_missing_cols(X_train, X_val, X_test, missing_threshold=0.5):\n",
    "    \"\"\"Удаляет колонки с большим процентом пропусков (на основе анализа train)\"\"\"\n",
    "    # Анализируем пропуски только в train\n",
    "    missing_ratios = X_train.isnull().mean()\n",
    "    high_missing_cols = missing_ratios[missing_ratios > missing_threshold].index.tolist()\n",
    "    \n",
    "    print(f\"Колонок с >{missing_threshold*100}% пропусков в train: {len(high_missing_cols)}\")\n",
    "    \n",
    "    if high_missing_cols:\n",
    "        print(\"\\nТоп-10 колонок с наибольшим процентом пропусков:\")\n",
    "        for col, perc in missing_ratios[high_missing_cols].sort_values(ascending=False).head(10).items():\n",
    "            print(f\"  {col}: {perc*100:.1f}%\")\n",
    "    \n",
    "    # Удаляем из всех датасетов\n",
    "    X_train_clean = X_train.drop(columns=high_missing_cols)\n",
    "    X_val_clean = X_val.drop(columns=[col for col in high_missing_cols if col in X_val.columns])\n",
    "    X_test_clean = X_test.drop(columns=[col for col in high_missing_cols if col in X_test.columns])\n",
    "    \n",
    "    print(f\"\\nРазмеры после удаления:\")\n",
    "    print(f\"Train: {X_train_clean.shape}\")\n",
    "    print(f\"Val: {X_val_clean.shape}\")\n",
    "    print(f\"Test: {X_test_clean.shape}\")\n",
    "    \n",
    "    \n",
    "    return X_train_clean, X_val_clean, X_test_clean, high_missing_cols\n",
    "\n",
    "# Применяем функцию\n",
    "X_train, X_val, X_test, high_missing_cols = remove_high_missing_cols(\n",
    "    X_train, X_val, X_test, missing_threshold=0.5\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe4df9",
   "metadata": {},
   "source": [
    "## Анализ и обработка выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac1654fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Обработка DAYS_EMPLOYED...\n",
      "   Создан признак DAYS_EMPLOYED_ANOM\n",
      "   Заменено 33122 значений 365243 на NaN в train\n",
      "\n",
      "2. Поиск признаков с выбросами...\n",
      "   Найдено признаков с выбросами: 359\n",
      "\n",
      "   Топ-15 признаков с выбросами:\n",
      "                                                  feature  n_outliers  outlier_perc\n",
      "293  client_installments_NUM_INSTALMENT_VERSION_mean_mean       42228     24.130286\n",
      "40                      bureau_AMT_CREDIT_MAX_OVERDUE_sum       37916     23.972434\n",
      "87               client_bureau_balance_STATUS_X_count_sum       31967     20.211172\n",
      "89          client_bureau_balance_STATUS_X_count_norm_sum       31608     19.984194\n",
      "83               client_bureau_balance_STATUS_C_count_sum       27747     17.543072\n",
      "29                         bureau_DAYS_CREDIT_ENDDATE_max       26867     17.136315\n",
      "55                          bureau_DAYS_CREDIT_UPDATE_max       26688     16.873518\n",
      "91                previous_NAME_PRODUCT_TYPE_walk-in_mean       29310     16.783481\n",
      "151                           previous_DAYS_FIRST_DUE_max       25435     14.632449\n",
      "67           client_bureau_balance_MONTHS_BALANCE_sum_sum       22941     14.504473\n",
      "326     client_installments_NUM_INSTALMENT_NUMBER_sum_sum       22584     12.905143\n",
      "148                           previous_DAYS_FIRST_DUE_sum       21164     12.118922\n",
      "31                         bureau_DAYS_CREDIT_ENDDATE_sum       19058     12.049442\n",
      "85          client_bureau_balance_STATUS_C_count_norm_sum       18959     11.986849\n",
      "215             client_cash_CNT_INSTALMENT_FUTURE_min_sum       20068     11.555115\n",
      "\n",
      "3. Обработка выбросов (winsorization 1%-99%)...\n",
      "   Обработано признаков: 359\n",
      "\n",
      "Размеры после обработки выбросов:\n",
      "Train: (184506, 853)\n",
      "Val: (61502, 853)\n",
      "Test: (61503, 853)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Обработка DAYS_EMPLOYED (специфичная аномалия)\n",
    "print(\"1. Обработка DAYS_EMPLOYED...\")\n",
    "days_employed_col = [col for col in X_train.columns if 'DAYS_EMPLOYED' in col]\n",
    "\n",
    "if days_employed_col:\n",
    "    col_name = days_employed_col[0]\n",
    "    \n",
    "    # Создаем признак аномалии\n",
    "    X_train[f'{col_name}_ANOM'] = (X_train[col_name] == 365243).astype(int)\n",
    "    X_val[f'{col_name}_ANOM'] = (X_val[col_name] == 365243).astype(int)\n",
    "    X_test[f'{col_name}_ANOM'] = (X_test[col_name] == 365243).astype(int)\n",
    "    \n",
    "    # Заменяем 365243 на NaN\n",
    "    X_train[col_name] = X_train[col_name].replace({365243: np.nan})\n",
    "    X_val[col_name] = X_val[col_name].replace({365243: np.nan})\n",
    "    X_test[col_name] = X_test[col_name].replace({365243: np.nan})\n",
    "    \n",
    "    print(f\"   Создан признак {col_name}_ANOM\")\n",
    "    print(f\"   Заменено {X_train[f'{col_name}_ANOM'].sum()} значений 365243 на NaN в train\")\n",
    "\n",
    "# 2. Находим все признаки с выбросами\n",
    "print(\"\\n2. Поиск признаков с выбросами...\")\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "outliers_list = []\n",
    "for col in numeric_cols:\n",
    "    data = X_train[col].dropna()\n",
    "    if len(data) > 10:\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        if IQR > 0:\n",
    "            lower = Q1 - 3 * IQR\n",
    "            upper = Q3 + 3 * IQR\n",
    "            \n",
    "            outliers = ((data < lower) | (data > upper)).sum()\n",
    "            if outliers > 0:\n",
    "                outlier_perc = outliers / len(data) * 100\n",
    "                outliers_list.append((col, outliers, outlier_perc))\n",
    "\n",
    "print(f\"   Найдено признаков с выбросами: {len(outliers_list)}\")\n",
    "\n",
    "# 3. Сортируем по проценту выбросов и показываем топ\n",
    "if outliers_list:\n",
    "    outliers_df = pd.DataFrame(outliers_list, columns=['feature', 'n_outliers', 'outlier_perc'])\n",
    "    outliers_df = outliers_df.sort_values('outlier_perc', ascending=False)\n",
    "    \n",
    "    print(\"\\n   Топ-15 признаков с выбросами:\")\n",
    "    print(outliers_df.head(15).to_string())\n",
    "    \n",
    "    # 4. Обрабатываем выбросы - простой метод winsorization (обрезаем)\n",
    "    print(\"\\n3. Обработка выбросов (winsorization 1%-99%)...\")\n",
    "    \n",
    "    for col in outliers_df['feature']:\n",
    "        if col in X_train.columns:\n",
    "            # Берем 1% и 99% перцентили из train\n",
    "            lower = X_train[col].quantile(0.01)\n",
    "            upper = X_train[col].quantile(0.99)\n",
    "            \n",
    "            # Обрабатываем все датасеты\n",
    "            X_train[col] = X_train[col].clip(lower, upper)\n",
    "            if col in X_val.columns:\n",
    "                X_val[col] = X_val[col].clip(lower, upper)\n",
    "            if col in X_test.columns:\n",
    "                X_test[col] = X_test[col].clip(lower, upper)\n",
    "    \n",
    "    print(f\"   Обработано признаков: {len(outliers_df)}\")\n",
    "\n",
    "print(\"\\nРазмеры после обработки выбросов:\")\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e2652",
   "metadata": {},
   "source": [
    "## Заполнение пропущенных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38339676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заполнение пропущенных значений...\n",
      "\n",
      "Результат импутации:\n",
      "Заполнено числовых колонок: 796\n",
      "Заполнено категориальных колонок: 3\n",
      "\n",
      "Пропусков после импутации:\n",
      "Train: 0\n",
      "Val: 0\n",
      "Test: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def impute_missing_no_leakage(X_train, X_val, X_test):\n",
    "    \"\"\"Заполняет пропуски без утечек данных (все значения из train)\"\"\"\n",
    "    print(\"Заполнение пропущенных значений...\")\n",
    "    \n",
    "    # Создаем копии\n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_val_imputed = X_val.copy()\n",
    "    X_test_imputed = X_test.copy()\n",
    "    \n",
    "    # Сохраняем значения для импутации\n",
    "    imputation_values = {\n",
    "        'numeric': {},\n",
    "        'categorical': {}\n",
    "    }\n",
    "    \n",
    "    # 1. Числовые колонки - заполняем медианой из train\n",
    "    numeric_cols = X_train_imputed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if X_train_imputed[col].isnull().any():\n",
    "            # Вычисляем медиану на train\n",
    "            fill_value = X_train_imputed[col].median()\n",
    "            imputation_values['numeric'][col] = fill_value\n",
    "            \n",
    "            # Заполняем train\n",
    "            X_train_imputed[col] = X_train_imputed[col].fillna(fill_value)\n",
    "            \n",
    "            # Заполняем val и test теми же значениями\n",
    "            if col in X_val_imputed.columns:\n",
    "                X_val_imputed[col] = X_val_imputed[col].fillna(fill_value)\n",
    "            if col in X_test_imputed.columns:\n",
    "                X_test_imputed[col] = X_test_imputed[col].fillna(fill_value)\n",
    "    \n",
    "    # 2. Категориальные колонки - заполняем 'MISSING'\n",
    "    categorical_cols = X_train_imputed.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if X_train_imputed[col].isnull().any():\n",
    "            fill_value = 'MISSING'\n",
    "            imputation_values['categorical'][col] = fill_value\n",
    "            \n",
    "            # Заполняем train\n",
    "            X_train_imputed[col] = X_train_imputed[col].fillna(fill_value)\n",
    "            \n",
    "            # Заполняем val и test теми же значениями\n",
    "            if col in X_val_imputed.columns:\n",
    "                X_val_imputed[col] = X_val_imputed[col].fillna(fill_value)\n",
    "            if col in X_test_imputed.columns:\n",
    "                X_test_imputed[col] = X_test_imputed[col].fillna(fill_value)\n",
    "    \n",
    "    # Проверяем результат\n",
    "    print(f\"\\nРезультат импутации:\")\n",
    "    print(f\"Заполнено числовых колонок: {len(imputation_values['numeric'])}\")\n",
    "    print(f\"Заполнено категориальных колонок: {len(imputation_values['categorical'])}\")\n",
    "    print(f\"\\nПропусков после импутации:\")\n",
    "    print(f\"Train: {X_train_imputed.isnull().sum().sum()}\")\n",
    "    print(f\"Val: {X_val_imputed.isnull().sum().sum()}\")\n",
    "    print(f\"Test: {X_test_imputed.isnull().sum().sum()}\")\n",
    "    \n",
    "    \n",
    "    return X_train_imputed, X_val_imputed, X_test_imputed, imputation_values\n",
    "\n",
    "# Заполняем пропуски\n",
    "X_train, X_val, X_test, imputation_values = impute_missing_no_leakage(X_train, X_val, X_test)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752954b9",
   "metadata": {},
   "source": [
    "## Обработка категорильных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a945a1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 13 категориальных признаков\n",
      "One-Hot Encoding для 12 признаков (≤20 уникальных)\n",
      "Label Encoding для 1 признаков (>20 уникальных)\n",
      "\n",
      "Применение Label Encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding: 100%|██████████| 1/1 [00:07<00:00,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Применение One-Hot Encoding...\n",
      "\n",
      "Размеры после кодирования:\n",
      "Train: (184506, 912)\n",
      "Val: (61502, 912)\n",
      "Test: (61503, 912)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_categorical_no_leakage(X_train, X_val, X_test, max_categories_ohe=20):\n",
    "    \"\"\"Кодирует категориальные признаки без утечек данных\"\"\"\n",
    "    # Определяем категориальные колонки\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if not categorical_cols:\n",
    "        print(\"Нет категориальных признаков для кодирования\")\n",
    "        return X_train, X_val, X_test, {}\n",
    "    \n",
    "    print(f\"Найдено {len(categorical_cols)} категориальных признаков\")\n",
    "    \n",
    "    # Разделяем на low и high cardinality\n",
    "    low_cardinality = []\n",
    "    high_cardinality = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_unique = X_train[col].nunique()\n",
    "        if n_unique <= max_categories_ohe:\n",
    "            low_cardinality.append(col)\n",
    "        else:\n",
    "            high_cardinality.append(col)\n",
    "    \n",
    "    print(f\"One-Hot Encoding для {len(low_cardinality)} признаков (≤{max_categories_ohe} уникальных)\")\n",
    "    print(f\"Label Encoding для {len(high_cardinality)} признаков (>{max_categories_ohe} уникальных)\")\n",
    "    \n",
    "    # Словарь для хранения энкодеров\n",
    "    encoders = {\n",
    "        'label_encoders': {},\n",
    "        'onehot_encoder': None,\n",
    "        'low_cardinality': low_cardinality,\n",
    "        'high_cardinality': high_cardinality\n",
    "    }\n",
    "    \n",
    "    # 1. Label Encoding для high cardinality\n",
    "    if high_cardinality:\n",
    "        print(\"\\nПрименение Label Encoding...\")\n",
    "        for col in tqdm(high_cardinality, desc=\"Label Encoding\"):\n",
    "            # Обучаем на train\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X_train[col].astype(str))\n",
    "            encoders['label_encoders'][col] = le\n",
    "            \n",
    "            # Применяем к train\n",
    "            X_train[col] = le.transform(X_train[col].astype(str))\n",
    "            \n",
    "            # Применяем к val (неизвестные -> -1)\n",
    "            if col in X_val.columns:\n",
    "                X_val[col] = X_val[col].astype(str).apply(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "            \n",
    "            # Применяем к test (неизвестные -> -1)\n",
    "            if col in X_test.columns:\n",
    "                X_test[col] = X_test[col].astype(str).apply(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "    \n",
    "    # 2. One-Hot Encoding для low cardinality\n",
    "    if low_cardinality:\n",
    "        print(\"\\nПрименение One-Hot Encoding...\")\n",
    "        \n",
    "        # Обучаем на train\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=np.int8)\n",
    "        ohe.fit(X_train[low_cardinality].astype(str))\n",
    "        encoders['onehot_encoder'] = ohe\n",
    "        \n",
    "        # Применяем ко всем датасетам\n",
    "        X_train_ohe = ohe.transform(X_train[low_cardinality].astype(str))\n",
    "        X_val_ohe = ohe.transform(X_val[low_cardinality].astype(str))\n",
    "        X_test_ohe = ohe.transform(X_test[low_cardinality].astype(str))\n",
    "        \n",
    "        # Создаем датафреймы\n",
    "        ohe_columns = ohe.get_feature_names_out(low_cardinality)\n",
    "        \n",
    "        X_train_ohe_df = pd.DataFrame(X_train_ohe, columns=ohe_columns, index=X_train.index)\n",
    "        X_val_ohe_df = pd.DataFrame(X_val_ohe, columns=ohe_columns, index=X_val.index)\n",
    "        X_test_ohe_df = pd.DataFrame(X_test_ohe, columns=ohe_columns, index=X_test.index)\n",
    "        \n",
    "        # Удаляем исходные колонки и добавляем OHE\n",
    "        X_train = X_train.drop(columns=low_cardinality)\n",
    "        X_val = X_val.drop(columns=low_cardinality)\n",
    "        X_test = X_test.drop(columns=low_cardinality)\n",
    "        \n",
    "        X_train = pd.concat([X_train, X_train_ohe_df], axis=1)\n",
    "        X_val = pd.concat([X_val, X_val_ohe_df], axis=1)\n",
    "        X_test = pd.concat([X_test, X_test_ohe_df], axis=1)\n",
    "    \n",
    "    print(f\"\\nРазмеры после кодирования:\")\n",
    "    print(f\"Train: {X_train.shape}\")\n",
    "    print(f\"Val: {X_val.shape}\")\n",
    "    print(f\"Test: {X_test.shape}\")\n",
    "    \n",
    "    \n",
    "    return X_train, X_val, X_test, encoders\n",
    "\n",
    "# Кодируем категориальные признаки\n",
    "X_train, X_val, X_test, categorical_encoders = encode_categorical_no_leakage(\n",
    "    X_train, X_val, X_test, max_categories_ohe=20\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0368ca",
   "metadata": {},
   "source": [
    "## Удаление сильно коррелированных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeb39e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск коррелированных признаков (порог: 0.95)...\n",
      "Найдено коррелированных признаков для удаления: 211\n",
      "\n",
      "Примеры удаляемых признаков:\n",
      "  AMT_GOODS_PRICE (корреляция 0.986 с AMT_CREDIT)\n",
      "  REGION_RATING_CLIENT_W_CITY (корреляция 0.951 с REGION_RATING_CLIENT)\n",
      "  YEARS_BEGINEXPLUATATION_MODE (корреляция 0.986 с YEARS_BEGINEXPLUATATION_AVG)\n",
      "  FLOORSMAX_MODE (корреляция 0.987 с FLOORSMAX_AVG)\n",
      "  YEARS_BEGINEXPLUATATION_MEDI (корреляция 0.996 с YEARS_BEGINEXPLUATATION_AVG)\n",
      "  FLOORSMAX_MEDI (корреляция 0.997 с FLOORSMAX_AVG)\n",
      "  OBS_60_CNT_SOCIAL_CIRCLE (корреляция 0.998 с OBS_30_CNT_SOCIAL_CIRCLE)\n",
      "  bureau_CREDIT_ACTIVE_Closed_count_norm (корреляция 0.992 с bureau_CREDIT_ACTIVE_Active_count_norm)\n",
      "  bureau_CREDIT_CURRENCY_currency 2_count_norm (корреляция 0.974 с bureau_CREDIT_CURRENCY_currency 1_count_norm)\n",
      "  bureau_CREDIT_TYPE_Interbank credit_count_norm (корреляция 1.000 с bureau_CREDIT_TYPE_Interbank credit_count)\n",
      "\n",
      "Размеры после удаления:\n",
      "Train: (184506, 701)\n",
      "Val: (61502, 701)\n",
      "Test: (61503, 701)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_correlated_features_no_leakage(X_train, X_val, X_test, correlation_threshold=0.95):\n",
    "    \"\"\"Удаляет сильно коррелированные признаки (анализ только на train)\"\"\"\n",
    "    print(f\"Поиск коррелированных признаков (порог: {correlation_threshold})...\")\n",
    "    \n",
    "    # Вычисляем корреляционную матрицу на train\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    \n",
    "    # Верхний треугольник матрицы\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Находим колонки для удаления\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > correlation_threshold)]\n",
    "    \n",
    "    print(f\"Найдено коррелированных признаков для удаления: {len(to_drop)}\")\n",
    "    \n",
    "    if to_drop:\n",
    "        print(\"\\nПримеры удаляемых признаков:\")\n",
    "        for col in to_drop[:10]:\n",
    "            # Найдем наиболее коррелированный признак\n",
    "            corr_values = upper[col]\n",
    "            if not corr_values.empty:\n",
    "                max_corr = corr_values.max()\n",
    "                corr_with = corr_values[corr_values == max_corr].index\n",
    "                if len(corr_with) > 0:\n",
    "                    print(f\"  {col} (корреляция {max_corr:.3f} с {list(corr_with)[0]})\")\n",
    "    \n",
    "    # Удаляем колонки из всех датасетов\n",
    "    X_train_uncorr = X_train.drop(columns=to_drop)\n",
    "    X_val_uncorr = X_val.drop(columns=[col for col in to_drop if col in X_val.columns])\n",
    "    X_test_uncorr = X_test.drop(columns=[col for col in to_drop if col in X_test.columns])\n",
    "    \n",
    "    print(f\"\\nРазмеры после удаления:\")\n",
    "    print(f\"Train: {X_train_uncorr.shape}\")\n",
    "    print(f\"Val: {X_val_uncorr.shape}\")\n",
    "    print(f\"Test: {X_test_uncorr.shape}\")\n",
    "    \n",
    "    \n",
    "    return X_train_uncorr, X_val_uncorr, X_test_uncorr, to_drop\n",
    "\n",
    "# Удаляем коррелированные признаки\n",
    "X_train, X_val, X_test, correlated_cols = remove_correlated_features_no_leakage(\n",
    "    X_train, X_val, X_test, correlation_threshold=0.95\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cd76217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранение обработанных данных...\n",
      "\n",
      "1. СОВПАДЕНИЕ КОЛОНОК:\n",
      "   Train колонок: 701\n",
      "   Val колонок: 701\n",
      "   Test колонок: 701\n",
      "   Совпадение train и val: True\n",
      "   Совпадение train и test: True\n",
      "\n",
      "2. ПРОВЕРКА ПРОПУСКОВ:\n",
      "   Train пропусков: 0\n",
      "   Val пропусков: 0\n",
      "   Test пропусков: 0\n",
      "\n",
      "3. ТИПЫ ДАННЫХ:\n",
      "   float64: 597 колонок\n",
      "   int8: 65 колонок\n",
      "   int64: 39 колонок\n",
      "\n",
      "4. СОХРАНЕНИЕ ДАННЫХ...\n",
      "   train_processed.csv: (184506, 703)\n",
      "   val_processed.csv: (61502, 703)\n",
      "   test_processed.csv: (61503, 703)\n",
      "\n",
      "Итоговые размеры:\n",
      "Train: (184506, 703)\n",
      "Val: (61502, 703)\n",
      "Test: (61503, 703)\n",
      "\n",
      "Всего признаков: 701\n",
      "Распределение таргета в train: 0.0807\n"
     ]
    }
   ],
   "source": [
    "def save_processed_data(X_train, X_val, X_test, train_ids, val_ids, test_ids, \n",
    "                       train_target, val_target, test_target):\n",
    "    \"\"\"Сохраняет обработанные данные\"\"\"\n",
    "    print(\"Сохранение обработанных данных...\")\n",
    "    \n",
    "    # 1. Проверяем совпадение колонок\n",
    "    train_cols = set(X_train.columns)\n",
    "    val_cols = set(X_val.columns)\n",
    "    test_cols = set(X_test.columns)\n",
    "    \n",
    "    print(f\"\\n1. СОВПАДЕНИЕ КОЛОНОК:\")\n",
    "    print(f\"   Train колонок: {len(train_cols)}\")\n",
    "    print(f\"   Val колонок: {len(val_cols)}\")\n",
    "    print(f\"   Test колонок: {len(test_cols)}\")\n",
    "    print(f\"   Совпадение train и val: {train_cols == val_cols}\")\n",
    "    print(f\"   Совпадение train и test: {train_cols == test_cols}\")\n",
    "    \n",
    "    # 2. Проверяем пропуски\n",
    "    print(f\"\\n2. ПРОВЕРКА ПРОПУСКОВ:\")\n",
    "    print(f\"   Train пропусков: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"   Val пропусков: {X_val.isnull().sum().sum()}\")\n",
    "    print(f\"   Test пропусков: {X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # 3. Проверяем типы данных\n",
    "    print(f\"\\n3. ТИПЫ ДАННЫХ:\")\n",
    "    dtype_counts = X_train.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   {dtype}: {count} колонок\")\n",
    "    \n",
    "    # 4. Собираем финальные датасеты\n",
    "    print(f\"\\n4. СОХРАНЕНИЕ ДАННЫХ...\")\n",
    "    \n",
    "    # Собираем полные датасеты\n",
    "    train_final = pd.concat([train_ids.reset_index(drop=True), \n",
    "                             train_target.reset_index(drop=True), \n",
    "                             X_train.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    val_final = pd.concat([val_ids.reset_index(drop=True), \n",
    "                           val_target.reset_index(drop=True), \n",
    "                           X_val.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    test_final = pd.concat([test_ids.reset_index(drop=True), \n",
    "                            test_target.reset_index(drop=True), \n",
    "                            X_test.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Сохраняем полные датасеты\n",
    "    train_final.to_csv('./data/train_processed.csv', index=False)\n",
    "    val_final.to_csv('./data/val_processed.csv', index=False)\n",
    "    test_final.to_csv('./data/test_processed.csv', index=False)\n",
    "    \n",
    "    print(f\"   train_processed.csv: {train_final.shape}\")\n",
    "    print(f\"   val_processed.csv: {val_final.shape}\")\n",
    "    print(f\"   test_processed.csv: {test_final.shape}\")\n",
    "    \n",
    "    \n",
    "    return train_final, val_final, test_final\n",
    "\n",
    "# Сохраняем данные\n",
    "train_final, val_final, test_final = save_processed_data(\n",
    "    X_train, X_val, X_test,\n",
    "    train_ids, val_ids, test_ids,\n",
    "    train_target, val_target, test_target\n",
    ")\n",
    "\n",
    "print(f\"\\nИтоговые размеры:\")\n",
    "print(f\"Train: {train_final.shape}\")\n",
    "print(f\"Val: {val_final.shape}\")\n",
    "print(f\"Test: {test_final.shape}\")\n",
    "print(f\"\\nВсего признаков: {X_train.shape[1]}\")\n",
    "print(f\"Распределение таргета в train: {train_target.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9781840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 184506 entries, 0 to 184505\n",
      "Columns: 703 entries, SK_ID_CURR to EMERGENCYSTATE_MODE_Yes\n",
      "dtypes: float64(597), int64(41), int8(65)\n",
      "memory usage: 909.5 MB\n"
     ]
    }
   ],
   "source": [
    "train_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42d99b",
   "metadata": {},
   "source": [
    "# Вывод\n",
    "\n",
    "Данные были обработаны. Удалены призанки с большим количеством пропусков. Обработаны выбросы. ЗАполнены пропущенные значения. Удалены сильно коррелирвоанные признаки. \n",
    "\n",
    "Готовые обработанные датасеты сохранены в файлы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
